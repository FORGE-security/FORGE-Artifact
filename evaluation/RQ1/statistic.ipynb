{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of FORGE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "from typing import List, Dict, Union, Optional, Literal\n",
    "from pydantic import BaseModel, RootModel, Field\n",
    "from collections import namedtuple,Counter\n",
    "\n",
    "@dataclass\n",
    "class ProjectInfo:\n",
    "    url: Union[str, int, List, None] = \"n/a\"\n",
    "    commit_id: Union[str, int, List, None] = \"n/a\"\n",
    "    address: Union[str, int, List, None] = \"n/a\"\n",
    "    chain: Union[str, int, List, None] = \"n/a\"\n",
    "    compiler_version: Union[str, List, None] = \"n/a\"\n",
    "    project_path: Union[str, List, Dict, None] = \"n/a\"\n",
    "\n",
    "    def is_empty(self):\n",
    "        if (self.url == \"n/a\" and self.address == \"n/a\") or (\n",
    "            not self.url and not self.address\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "@dataclass\n",
    "class Finding:\n",
    "    id: Union[str, int] = 0\n",
    "    category: Dict = field(default_factory=dict)\n",
    "    title: str = \"\"\n",
    "    description: str = \"\"\n",
    "    severity: str = \"\"\n",
    "    location: Union[str, int, List] = \"\"\n",
    "\n",
    "\n",
    "class Report(BaseModel):\n",
    "    path: str = \"\"\n",
    "    project_info: ProjectInfo = field(default_factory=ProjectInfo)\n",
    "    findings: List[Finding] = field(default_factory=list)\n",
    "\n",
    "    def append_finding(self, finding: Finding):\n",
    "        self.findings.append(finding)\n",
    "\n",
    "\n",
    "class JSONFileProcessor:\n",
    "    def __init__(self, directory: str):\n",
    "        self.directory = directory\n",
    "        self.file_count = 0\n",
    "\n",
    "    def _get_all_json_files(self) -> List[str]:\n",
    "        json_files = []\n",
    "        for root, _, files in os.walk(self.directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    json_files.append(os.path.join(root, file))\n",
    "                    self.file_count += 1\n",
    "        return json_files\n",
    "\n",
    "    def operate_add(self,results:List,result_type):\n",
    "        res = {}\n",
    "        for field in result_type._fields:\n",
    "            if isinstance(getattr(results[0],field),int):\n",
    "                res[field] = 0\n",
    "            else:\n",
    "                res[field] = []\n",
    "            # res[field] = 0\n",
    "        for result in results:\n",
    "            for field in result._fields:\n",
    "                res[field] += getattr(result, field)\n",
    "\n",
    "        return res\n",
    "    def operate_reduce(self,results:List,result_type):\n",
    "        res = {}\n",
    "        for field in result_type._fields:\n",
    "            res[field] = []\n",
    "        for result in results:\n",
    "            for field in result._fields:\n",
    "                res[field].extend(getattr(result, field))\n",
    "        return res\n",
    "\n",
    "    def process_files(\n",
    "        self, analysis_func=None\n",
    "    ) -> List[Report]:\n",
    "        results = []\n",
    "        json_files = self._get_all_json_files()\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, \"r\", encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                report = Report(**data)\n",
    "                if analysis_func:\n",
    "                    result = analysis_func(report)\n",
    "                    results.append(result)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count findings & projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_files': 6454, 'total_projects': 6579, 'total_findings': 27497}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Finding = namedtuple(\"Finding\", [\"total_files\",\"total_projects\",\"total_findings\"])\n",
    "\n",
    "def count_finding(report: Report):\n",
    "    result = Finding(total_findings=len(report.findings),total_files=1,total_projects=len(report.project_info.project_path.keys()))\n",
    "    return result\n",
    "\n",
    "processor = JSONFileProcessor(\"../../dataset/results\")\n",
    "results = processor.process_files(analysis_func=count_finding)\n",
    "res = processor.operate_add(results,Finding)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check & count contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "Result = namedtuple(\n",
    "    \"Result\",\n",
    "    [\n",
    "        \"total_files\",\n",
    "        \"total_projects\",\n",
    "        \"valid_projects\",\n",
    "        \"solidity_files\",\n",
    "        \"lines_of_code\",\n",
    "        \"compiler_version\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "def check_projects(report: Report):\n",
    "    def count_lines_and_get_compiler_version(filepath: Path):\n",
    "        regex = re.compile(r\"pragma [^;]+( [^;]+)*;\", re.IGNORECASE|re.MULTILINE)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as sol_file:\n",
    "                lines =  len(sol_file.readlines())\n",
    "                sol_file.seek(0)\n",
    "                for line in sol_file:\n",
    "                    match = regex.match(line)\n",
    "                    if match:\n",
    "                        return (lines, match.group(0))\n",
    "                return (lines, \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening file {filepath}: {e}\")\n",
    "            return (0,\"\")\n",
    "\n",
    "    def deal_compiler_version(version_list:list):\n",
    "        version_set = set(version_list)\n",
    "        version_dict = {}\n",
    "        for version in version_set:\n",
    "            version_dict[version] = version_list.count(version)\n",
    "        if \"\" in version_dict:\n",
    "            del version_dict[\"\"]\n",
    "        if not version_dict:\n",
    "            return \"\"\n",
    "        return max(version_dict, key=version_dict.get)\n",
    "    if report.project_info.is_empty():\n",
    "        valid_projects = 0\n",
    "        print(f\"Empty project info in {report.path}\")\n",
    "    else:\n",
    "        valid_projects = len(report.project_info.project_path.keys())\n",
    "        solidity_files = 0\n",
    "        lines_of_code = 0\n",
    "        compiler_versions = []\n",
    "        for k, v in report.project_info.project_path.items():\n",
    "            paths = Path(\"../../dataset/\" + v).glob(\"**/*.sol\")\n",
    "            for path in paths:\n",
    "\n",
    "                line,version= count_lines_and_get_compiler_version(path)\n",
    "                lines_of_code += line\n",
    "                compiler_versions.append(version)\n",
    "            solidity_files += len(list(Path(\"../../dataset/\" + v).glob(\"**/*.sol\")))\n",
    "    compiler_version = deal_compiler_version(compiler_versions)\n",
    "    # if compiler_version == \"\":\n",
    "        # print(f\"No compiler version found in {report.path}\")\n",
    "    return Result(\n",
    "        total_files=1,\n",
    "        total_projects=len(report.project_info.project_path.keys()),\n",
    "        valid_projects=valid_projects,\n",
    "        solidity_files=solidity_files,\n",
    "        lines_of_code=lines_of_code,\n",
    "        compiler_version=[compiler_version]\n",
    "    )\n",
    "\n",
    "\n",
    "processor = JSONFileProcessor(\"../../dataset/results\")\n",
    "results = processor.process_files(analysis_func=check_projects)\n",
    "res = processor.operate_add(results, Result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 6454\n",
      "Total projects: 6579\n",
      "Valid projects: 6579\n",
      "Total solidity files: 81390\n",
      "Total lines of code: 16941428\n",
      "Average lines of code: 2575.076455388357\n",
      "Average files per project: 12.371181030551755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'^0.8': 3788, '^0.6': 1518, '^0.4': 275, '^0.5': 480, '^0.7': 362})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_lines_of_code = res[\"lines_of_code\"] / res[\"valid_projects\"]\n",
    "average_files_per_project = res[\"solidity_files\"] / res[\"valid_projects\"]\n",
    "print(f\"Total files: {res['total_files']}\")\n",
    "print(f\"Total projects: {res['total_projects']}\")\n",
    "print(f\"Valid projects: {res['valid_projects']}\")\n",
    "print(f\"Total solidity files: {res['solidity_files']}\")\n",
    "print(f\"Total lines of code: {res['lines_of_code']}\")\n",
    "print(f\"Average lines of code: {average_lines_of_code}\")\n",
    "print(f\"Average files per project: {average_files_per_project}\")\n",
    "# some might be pragma solidity >=0.7.0 <0.8.0;\n",
    "from collections import Counter\n",
    "compiler_versions = res[\"compiler_version\"]\n",
    "compiler_versions\n",
    "compiler_versions2 = []\n",
    "# compiler_versions = [version for version in compiler_versions if version]\n",
    "regex = re.compile(r\"\\d+\\.\\d+(\\.\\d+)?\", re.IGNORECASE)\n",
    "for version in compiler_versions:\n",
    "    # print(version)\n",
    "    version2 = regex.search(version).group(0) if regex.search(version) else \"\"\n",
    "    \n",
    "    if not version2:\n",
    "        continue\n",
    "    if version2 < \"0.5.0\":\n",
    "        compiler_versions2.append(\"^0.4\")\n",
    "    elif version2 < \"0.6.0\":\n",
    "        compiler_versions2.append(\"^0.5\")\n",
    "    elif version2 < \"0.7.0\":\n",
    "        compiler_versions2.append(\"^0.6\")\n",
    "    elif version2 < \"0.8.0\":\n",
    "        compiler_versions2.append(\"^0.7\")\n",
    "    elif version2 < \"0.9.0\":\n",
    "        compiler_versions2.append(\"^0.8\")\n",
    "\n",
    "compiler_version = Counter(compiler_versions2)\n",
    "compiler_version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'na': 2406,\n",
       " 'info': 3456,\n",
       " 'low': 15170,\n",
       " 'medium': 3367,\n",
       " 'high': 2094,\n",
       " 'critical': 1004}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_category = [\"na\",\"info\",\"low\",\"medium\",\"high\",\"critical\"]\n",
    "\n",
    "Severity = namedtuple(\"Category\",severity_category)\n",
    "def count_severity(report: Report):\n",
    "    findings = report.findings\n",
    "    severity_dict = {\"na\": 0, \"info\": 0, \"low\": 0, \"medium\": 0, \"high\": 0, \"critical\": 0}\n",
    "    for finding in findings:\n",
    "        severity = finding.severity.lower().strip()\n",
    "        if severity == \"n/a\":\n",
    "            severity = \"na\"\n",
    "        if severity not in severity_category:\n",
    "            continue\n",
    "        severity_dict[severity] = severity_dict.get(severity, 0) + 1\n",
    "        # dict to namedtuple\n",
    "    return Severity(**severity_dict)\n",
    "\n",
    "processor = JSONFileProcessor(\"../../dataset/results\")\n",
    "results = processor.process_files(analysis_func=count_severity)\n",
    "res = processor.operate_add(results, Severity)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CWE-Id statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique CWE IDs: 296\n"
     ]
    }
   ],
   "source": [
    "CWE = namedtuple(\"CWE\", [\"cwe_ids\"])\n",
    "\n",
    "def extract_cwe_ids(report: Report):\n",
    "    cwe_ids = []\n",
    "    for finding in report.findings:\n",
    "        if finding.category:\n",
    "            # Get the ID with the highest key from the category dictionary\n",
    "            try:\n",
    "                max_key = max([int(k) for k in finding.category.keys() if k.isdigit()])\n",
    "                cwe_value = finding.category.get(str(max_key), [\"\"])[0]\n",
    "                # Extract CWE ID if it exists\n",
    "                if isinstance(cwe_value, str) and cwe_value.startswith(\"CWE-\"):\n",
    "                    cwe_ids.append(cwe_value)\n",
    "            except (ValueError, AttributeError):\n",
    "                continue\n",
    "    return CWE(cwe_ids=[cwe_id for cwe_id in cwe_ids])\n",
    "\n",
    "processor = JSONFileProcessor(\"../../dataset/results\")\n",
    "results = processor.process_files(analysis_func=extract_cwe_ids)\n",
    "cwe_result = processor.operate_reduce(results, CWE)\n",
    "\n",
    "# Count unique CWE IDs\n",
    "unique_cwe_ids = set(cwe_result[\"cwe_ids\"])\n",
    "cwe_counter = Counter(cwe_result[\"cwe_ids\"])\n",
    "\n",
    "print(f\"Total unique CWE IDs: {len(unique_cwe_ids)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
